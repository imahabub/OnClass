{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/groups/rbaltman/swang91/tools/miniconda/miniconda/envs/tabula-muris-env/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from scipy import stats, sparse\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from OnClass.OnClassModel import OnClassModel\n",
    "from utils import read_ontology_file, read_data, make_folder, read_data_file, find_gene_ind\n",
    "from config import ontology_data_dir, scrna_data_dir, result_dir\n",
    "\n",
    "\n",
    "dname1 = 'microcebusBernard'\n",
    "dname2 = 'microcebusAntoine'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cell_type_nlp_emb_file, cell_type_network_file, cl_obo_file = read_ontology_file(dname1, ontology_data_dir)\n",
    "OnClass_train_obj = OnClassModel(cell_type_nlp_emb_file = cell_type_nlp_emb_file, cell_type_network_file = cell_type_network_file)\n",
    "feature_file, filter_key, label_key, label_file, gene_file = read_data_file(dname1, scrna_data_dir)\n",
    "if feature_file.endswith('.pkl'):\n",
    "    feature1, label1, genes1 = parse_pkl(feature_file, label_file, gene_file, exclude_non_leaf_ontology = True, cell_ontology_file = cell_type_network_file)\n",
    "elif feature_file.endswith('.h5ad'):\n",
    "    feature1, genes1, label1, _, _ = read_data(feature_file, cell_ontology_ids = OnClass_train_obj.cell_ontology_ids,\n",
    "    exclude_non_leaf_ontology = True, tissue_key = 'tissue', filter_key = filter_key, AnnData_label_key=label_key,\n",
    "    nlp_mapping = False, cl_obo_file = cl_obo_file, cell_ontology_file = cell_type_network_file, co2emb = OnClass_train_obj.co2vec_nlp)\n",
    "\n",
    "feature_file, filter_key, label_key, label_file, gene_file = read_data_file(dname2, scrna_data_dir)\n",
    "if feature_file.endswith('.pkl'):\n",
    "    feature2, label2, genes2 = parse_pkl(feature_file, label_file, gene_file, exclude_non_leaf_ontology = True, cell_ontology_file = cell_type_network_file)\n",
    "elif feature_file.endswith('.h5ad'):\n",
    "    feature2, genes2, label2, _, _ = read_data(feature_file, cell_ontology_ids = OnClass_train_obj.cell_ontology_ids,\n",
    "    exclude_non_leaf_ontology = True, tissue_key = 'tissue', filter_key = filter_key, AnnData_label_key=label_key,\n",
    "    nlp_mapping = False, cl_obo_file = cl_obo_file, cell_ontology_file = cell_type_network_file, co2emb = OnClass_train_obj.co2vec_nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed cell types based on the Cell Ontology Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17614, 31509) (75501, 31509)\n",
      "(75501, 31509) (17614, 31509)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(feature1), np.shape(feature2))\n",
    "feature1 = np.array(feature1, dtype=np.float64)\n",
    "feature2 = np.array(feature2, dtype=np.float64)\n",
    "common_genes = np.sort(list(set(genes1) & set(genes2)))\n",
    "gid1 = find_gene_ind(genes1, common_genes)\n",
    "gid2 = find_gene_ind(genes2, common_genes)\n",
    "train_feature = np.array(feature1[:, gid1])\n",
    "test_feature = np.array(feature2[:, gid2])\n",
    "train_label = label1\n",
    "test_label = label2\n",
    "train_genes = common_genes\n",
    "test_genes = common_genes\n",
    "print (np.shape(test_feature), np.shape(train_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/oak/stanford/groups/rbaltman/swang91/Sheng_repo/result/SingleCell/OnClass/Crossdatasets/'\n",
    "pred_Y_seen = np.load(output_dir+dname1+'.'+dname2 + 'pred_Y_seen.npy')\n",
    "pred_Y_all = np.load(output_dir+dname1+'.'+dname2 + 'pred_Y_all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OnClass_train_obj.EmbedCellTypes(train_label)\n",
    "train_label = [OnClass_train_obj.co2i[tp] for tp in train_label]\n",
    "test_label = [OnClass_train_obj.co2i[tp] for tp in test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 52.39630212,  55.2635893 , 156.84920896, ..., 123.93616349,\n",
       "        22.79872163,   1.74655316])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_Y_all_new[:,80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import shortest_path as graph_shortest_path\n",
    "from utils import get_ontology_name, precision_at_k\n",
    "from OnClass.OnClass_utils import create_propagate_networks_using_nlp, extend_prediction_2unseen\n",
    "\n",
    "nct = len(OnClass_train_obj.co2i)\n",
    "A = np.zeros((nct, nct))\n",
    "fin = open(OnClass_train_obj.cell_type_network_file)\n",
    "for line in fin:\n",
    "    w = line.strip().split('\\t')\n",
    "    A[OnClass_train_obj.co2i[w[0]], OnClass_train_obj.co2i[w[1]]] = 1.\n",
    "    A[OnClass_train_obj.co2i[w[1]], OnClass_train_obj.co2i[w[0]]] = 1.\n",
    "fin.close()\n",
    "A_dis = graph_shortest_path(A,method='FW',directed =False)\n",
    "    \n",
    "def create_unseen_candidates(A_dis, co2i, i2co, nseen, use_unseen_distance, test_Y_pred_all):\n",
    "\n",
    "\tmin_d = np.min(A_dis[:nseen, nseen:], axis = 0)\n",
    "\tassert(len(min_d) == nct - nseen)\n",
    "\tunseen_cand = np.where(min_d > use_unseen_distance)[0] + nseen\n",
    "\ttest_Y_pred_all[:, unseen_cand] = -1000000\n",
    "\tassert(np.shape(test_Y_pred_all)[1] == nct)\n",
    "\treturn test_Y_pred_all\n",
    "\n",
    "co2name, name2co = get_ontology_name(obo_file = cl_obo_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'networks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-ac002c901e37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m network = create_propagate_networks_using_nlp(OnClass_train_obj.co2i, OnClass_train_obj.ontology_dict, \n\u001b[1;32m      2\u001b[0m                                               OnClass_train_obj.ontology_mat, OnClass_train_obj.co2vec_nlp_mat)\n\u001b[0;32m----> 3\u001b[0;31m pred_Y_all_normalize = extend_prediction_2unseen(pred_Y_seen, networks, \n\u001b[0m\u001b[1;32m      4\u001b[0m                                                  \u001b[0mOnClass_train_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnseen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                  ratio=200, use_normalize = True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'networks' is not defined"
     ]
    }
   ],
   "source": [
    "network = create_propagate_networks_using_nlp(OnClass_train_obj.co2i, OnClass_train_obj.ontology_dict, \n",
    "                                              OnClass_train_obj.ontology_mat, OnClass_train_obj.co2vec_nlp_mat)\n",
    "pred_Y_all_normalize = extend_prediction_2unseen(pred_Y_seen, network, \n",
    "                                                 OnClass_train_obj.nseen, \n",
    "                                                 ratio=200, use_normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46735804823777166 0.5398868889153786 0.5469066634879008 0.5625355955550257\n",
      "2 0.0001986728652600628 0.000860915749460272 0.0019867286526006277 0.0029006238327969167\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-7f70d3be740b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_unseen_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_at_k_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_at_k_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_at_k_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_at_k_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0munseen_in_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mOnClass_train_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnseen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sd' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "ratio = 1#(OnClass_train_obj.nco*1./OnClass_train_obj.nseen)**2\n",
    "ntest_cell = len(test_label)\n",
    "test_label = np.array(test_label)\n",
    "\n",
    "pred_Y_all_normalize = extend_prediction_2unseen(pred_Y_seen, network, \n",
    "                                             OnClass_train_obj.nseen, \n",
    "                                             ratio=ratio, use_normalize = False)\n",
    "\n",
    "prec_at_k_1 = precision_at_k(pred_Y_all_normalize, test_label, 1)\n",
    "prec_at_k_3 = precision_at_k(pred_Y_all_normalize, test_label, 3)\n",
    "prec_at_k_5 = precision_at_k(pred_Y_all_normalize, test_label, 5)\n",
    "prec_at_k_10 = precision_at_k(pred_Y_all_normalize, test_label, 10)\n",
    "#print (np.unique(pred_label))\n",
    "print (prec_at_k_1, prec_at_k_3, prec_at_k_5, prec_at_k_10)\n",
    "\n",
    "\n",
    "unseen_confidence = np.max(pred_Y_all_normalize1[:,OnClass_train_obj.nseen:], axis=1) - np.max(pred_Y_all_normalize1[:,:OnClass_train_obj.nseen], axis=1)\n",
    "\n",
    "unseen_ratio = 0.00001\n",
    "nexpected_unseen = int(ntest_cell * unseen_ratio) + 1\n",
    "unseen_ind = np.argpartition(unseen_confidence, -1 * nexpected_unseen)[-1 * nexpected_unseen:]\n",
    "\n",
    "\n",
    "pred_Y_all_normalize[:,OnClass_train_obj.nseen:] = stats.zscore(pred_Y_all_normalize[:,OnClass_train_obj.nseen:], \n",
    "                                                                axis = 0)  \n",
    "\n",
    "pred_Y_all_normalize[unseen_ind,:OnClass_train_obj.nseen] -= 1000000\n",
    "\n",
    "prec_at_k_1 = precision_at_k(pred_Y_all_normalize, test_label, 1)\n",
    "prec_at_k_3 = precision_at_k(pred_Y_all_normalize, test_label, 3)\n",
    "prec_at_k_5 = precision_at_k(pred_Y_all_normalize, test_label, 5)\n",
    "prec_at_k_10 = precision_at_k(pred_Y_all_normalize, test_label, 10)\n",
    "#print (np.unique(pred_label))\n",
    "\n",
    "print (use_unseen_distance, prec_at_k_1, prec_at_k_3, prec_at_k_5, prec_at_k_10)\n",
    "sd\n",
    "sd\n",
    "unseen_in_test = np.where(np.array(test_label) >= OnClass_train_obj.nseen)[0]\n",
    "pred_label = np.argmax(pred_Y_all_normalize, axis=1)\n",
    "depth = []\n",
    "for i in np.unique(pred_label):\n",
    "    depth.append(A_dis[OnClass_train_obj.co2i['CL:0000000'], i])\n",
    "print (Counter(depth))\n",
    "prec_at_k_3 = precision_at_k(pred_Y_all_normalize[unseen_in_test,:], test_label[unseen_in_test], 3)\n",
    "prec_at_k_5 = precision_at_k(pred_Y_all_normalize[unseen_in_test,:], test_label[unseen_in_test], 5)\n",
    "prec_at_k_10 = precision_at_k(pred_Y_all_normalize[unseen_in_test,:], test_label[unseen_in_test], 10)\n",
    "#print (np.unique(pred_label))\n",
    "\n",
    "print (use_unseen_distance, prec_at_k_3, prec_at_k_5, prec_at_k_10)\n",
    "sd\n",
    "\n",
    "unseen_conf = np.max(pred_Y_all_normalize1[:,OnClass_train_obj.nseen:], axis=1) - np.max(pred_Y_all_normalize1[:,:OnClass_train_obj.nseen], axis=1)\n",
    "\n",
    "#unseen_ratio = 0.1\n",
    "expected_unseen = int(ntest_cell * unseen_ratio)\n",
    "unseen_ind = np.argpartition(unseen_conf, -1 * expected_unseen)[-1 * expected_unseen:]\n",
    "\n",
    "pred_label = np.argmax(pred_Y_all_normalize1, axis=1)\n",
    "pred_label[unseen_ind] = np.argmax(pred_Y_all_normalize1[unseen_ind,OnClass_train_obj.nseen:], axis=1)\n",
    "pred_Y_all_normalize1[unseen_ind,:OnClass_train_obj.nseen] = -1000000\n",
    "\n",
    "\n",
    "unseen_in_test = np.where(np.array(test_label) >= OnClass_train_obj.nseen)[0]\n",
    "prec_at_k_3 = precision_at_k(pred_Y_all_normalize1[unseen_in_test,:], test_label[unseen_in_test], 3)\n",
    "prec_at_k_5 = precision_at_k(pred_Y_all_normalize1[unseen_in_test,:], test_label[unseen_in_test], 5)\n",
    "prec_at_k_10 = precision_at_k(pred_Y_all_normalize1[unseen_in_test,:], test_label[unseen_in_test], 10)\n",
    "#print (np.unique(pred_label))\n",
    "\n",
    "print (use_normalize, unseen_ratio, use_unseen_distance, prec_at_k_3, prec_at_k_5, prec_at_k_10)\n",
    "'''\n",
    "act = Counter(pred_label)\n",
    "#for ct in act:\n",
    "#    nct = act[ct]\n",
    "#    print (ct, co2name[OnClass_train_obj.i2co[ct]], nct)\n",
    "\n",
    "unseen_acc = 0.\n",
    "seen_acc = 0.\n",
    "for i in range(ntest_cell):\n",
    "    if pred_label[i] == test_label[i]:\n",
    "        if pred_label[i] < OnClass_train_obj.nco:\n",
    "            seen_acc += 1\n",
    "        else:\n",
    "            unseen_acc += 1\n",
    "print (use_normalize, unseen_ratio, unseen_acc / ntest_cell, seen_acc / ntest_cell) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unseen_ratio = 0.00001\n",
    "nexpected_unseen = int(ntest_cell * unseen_ratio) + 1\n",
    "unseen_ind = np.argpartition(unseen_confidence, -1 * nexpected_unseen)[-1 * nexpected_unseen:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify test cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32804])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.0001986728652600628 0.000860915749460272 0.0019867286526006277 0.0029006238327969167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unseen_ratio = 0.00001\n",
    "nexpected_unseen = int(ntest_cell * unseen_ratio) + 1\n",
    "unseen_ind = np.argpartition(unseen_confidence, -1 * nexpected_unseen)[-1 * nexpected_unseen:]\n",
    "seen_ind = np.argpartition(unseen_confidence, -1 * nexpected_unseen)[:-1 * nexpected_unseen]\n",
    "\n",
    "pred_Y_all_normalize[:,OnClass_train_obj.nseen:] = stats.zscore(pred_Y_all_normalize[:,OnClass_train_obj.nseen:], \n",
    "                                                                axis = 0)  \n",
    "\n",
    "pred_Y_all_normalize[unseen_ind,:OnClass_train_obj.nseen] -= 1000000\n",
    "pred_Y_all_normalize[seen_ind,:OnClass_train_obj.nseen] -= 1000000\n",
    "prec_at_k_1 = precision_at_k(pred_Y_all_normalize, test_label, 1)\n",
    "prec_at_k_3 = precision_at_k(pred_Y_all_normalize, test_label, 3)\n",
    "prec_at_k_5 = precision_at_k(pred_Y_all_normalize, test_label, 5)\n",
    "prec_at_k_10 = precision_at_k(pred_Y_all_normalize, test_label, 10)\n",
    "#print (np.unique(pred_label))\n",
    "\n",
    "print (use_unseen_distance, prec_at_k_1, prec_at_k_3, prec_at_k_5, prec_at_k_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001986728652600628 0.000860915749460272 0.0019867286526006277 0.0029006238327969167\n",
      "0.0001986728652600628 0.000860915749460272 0.0019867286526006277 0.0029006238327969167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pred_Y_all_normalize = stats.zscore(pred_Y_all_normalize, axis = 0)  \n",
    "\n",
    "prec_at_k_1 = precision_at_k(pred_Y_all_normalize, test_label, 1)\n",
    "prec_at_k_3 = precision_at_k(pred_Y_all_normalize, test_label, 3)\n",
    "prec_at_k_5 = precision_at_k(pred_Y_all_normalize, test_label, 5)\n",
    "prec_at_k_10 = precision_at_k(pred_Y_all_normalize, test_label, 10)\n",
    "#print (np.unique(pred_label))\n",
    "\n",
    "print (prec_at_k_1, prec_at_k_3, prec_at_k_5, prec_at_k_10)\n",
    "\n",
    "unseen_confidence = np.max(pred_Y_all_normalize1[:,OnClass_train_obj.nseen:], axis=1) - np.max(pred_Y_all_normalize1[:,:OnClass_train_obj.nseen], axis=1)\n",
    "\n",
    "unseen_ratio = 0.00001\n",
    "nexpected_unseen = int(ntest_cell * unseen_ratio)\n",
    "unseen_ind = np.argpartition(unseen_confidence, -1 * nexpected_unseen)[-1 * nexpected_unseen:]\n",
    "\n",
    "pred_Y_all_normalize[unseen_ind,:OnClass_train_obj.nseen] -= 1000000\n",
    "pred_label = np.argmax(pred_Y_all_normalize1, axis=1)\n",
    "\n",
    "prec_at_k_1 = precision_at_k(pred_Y_all_normalize, test_label, 1)\n",
    "prec_at_k_3 = precision_at_k(pred_Y_all_normalize, test_label, 3)\n",
    "prec_at_k_5 = precision_at_k(pred_Y_all_normalize, test_label, 5)\n",
    "prec_at_k_10 = precision_at_k(pred_Y_all_normalize, test_label, 10)\n",
    "#print (np.unique(pred_label))\n",
    "\n",
    "print (prec_at_k_1, prec_at_k_3, prec_at_k_5, prec_at_k_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.46733155852240366\n"
     ]
    }
   ],
   "source": [
    "pred_Y_all[:, OnClass_train_obj.nseen:] -= 1000000\n",
    "pred_label = np.argmax(pred_Y_all, axis=1)\n",
    "unseen_acc = 0.\n",
    "seen_acc = 0.\n",
    "for i in range(ntest_cell):\n",
    "    if pred_label[i] == test_label[i]:\n",
    "        if pred_label[i] < OnClass_train_obj.nseen:\n",
    "            seen_acc += 1\n",
    "        else:\n",
    "            unseen_acc += 1\n",
    "print (unseen_acc / ntest_cell, seen_acc / ntest_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5476881100912571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prec_at_k_1 = precision_at_k(pred_Y_all, test_label, 3)\n",
    "print (prec_at_k_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 10,  1, ...,  1, 23, 23])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 513,   10,   39, ...,   39, 1092,   15])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tabula-muris-env)",
   "language": "python",
   "name": "tabula-muris-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
